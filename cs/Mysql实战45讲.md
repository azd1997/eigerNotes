---
title: "《Mysql实战45讲》速览"
date: 2020-09-04T13:45:49+08:00
draft: false
categories: ["cs"]
tags: ["数据库"]
keywords: ["Mysql"]
---

## 1. 基础架构：一条SQL查询语句是如何执行的？

![mysql45-01-1](/images/mysql45-01-1.png)

一条查询SQL语句`mysql> select * from T where ID=10；`的执行流程：

1. 客户端将请求打给server层
2. **连接器**负责管理连接，权限验证，检查该客户端是否有执行该SQL权限，有则建立连接并继续
3. 首先看**查询缓存**中是否有，有则返回；
4. 无则将SQL交给**分析器**，对SQL语句进行词法分析、语法分析
5. 继续交给**优化器**，负责执行计划生成，索引选择
6. 继续交给**执行器**，执行器操作**存储引擎**（存储引擎独立于server层，负责存储数据，提供读写接口），返回结果。

大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。

- Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。
- 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。

### 1.1 连接器

跟客户端建立连接、获取权限、维持和管理连接

```shell
mysql -h$ip -P$port -u$user -p
**** # 输入密码
```

连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。

- 如果用户名或密码不对，你就会收到一个"Access denied for user"的错误，然后客户端程序结束执行。
- 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。*之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限*。这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。

建立连接后若没有后续动作，则连接处于空闲状态，使用`show processlist`可看到该链接`Command`列为`Sleep`

空闲太长时间（超过`wait_timeout`（默认8h）），则连接会被连接器断开。

由于建立连接的过程比较复杂，尽量使用长连接。
    - 全使用长连接后，Mysql内存占用涨得很快，因为MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了
    - 解决方法：
        - 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后， 断开连接，之后要查询再重连。
        - 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行`mysql_reset_connection` 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

### 1.2 查询缓存

查询缓存的形式为 <查询语句, 查询结果>。若直接命中，则取缓存结果直接返回。

*不建议使用查询缓存*
    - 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。
    - 对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。

MySQL 也提供了这种“按需使用”的方式。可以将参数 `query_cache_type` 设置成 `DEMAND`，这样对于默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 `SQL_CACHE` 显式指定，像下面这个语句一样：

```sql
mysql> select SQL_CACHE * from T where ID=10；
```

- MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻
底没有这个功能了。

### 1.3 分析器

MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析
    - 分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。
    - “语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法

- 如果语句不对，就会收到“You have an error in your SQL syntax”的错误，一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。

### 1.4 优化器

经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。

- 优化器是在表里面有多个索引的时候，决定使用哪个索引
- 或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。

优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段

### 1.5 执行器

MySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。

1. 开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误。 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。
    - 也就是说：命中查询缓存返回之前、优化器之前会precheck、执行器执行之前 都有做权限检查
2. 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口，扫描*所有可能存在结果的数据行*(对于没索引情况，则是顺序遍历所有行，有索引则是借助索引树遍历所有符合索引条件的行)。 

你会在数据库的慢查询日志中看到一个 `rows_examined` 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此*引擎扫描行数跟`rows_examined` 并不是完全相同的*。

### 提问

如果表 T 中没有字段 k，而你执行了这个语句 select * from T
where k=1, 那肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是在我们上面提到的哪个阶段报出来的呢？

*A: 分析器。在分析阶段判断语句是否正确，表是否存在，列是否存在等*

## 2. 日志系统：一条SQL更新语句是如何执行的？

考虑更新语句如下：

```sql
// ID主键， c整型
mysql> update T set c=c+1 where ID=2;
```

更新语句和查询语句一样需要经过“连接器->分析器->优化器->执行器”，但还涉及两个重要的日志模块：**redo log（重做日志）和 binlog（归档日志）**

- 别忘了，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空

### 2.1 redo log

如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。

因此，Mysql采用**WAL（Write-Ahead Log）**技术。**先写日志，再写磁盘**。 

**InnoDB有redolog。（换句话说redolog是InnoDB特有的日志）**

具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。

InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。

![mysql45-02-1](/images/mysql45-02-1.png)

- write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。

- write pos 和 checkpoint 之间的是还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示四个redolog没有可写的位置，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。
- 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。或者叫CFT（Crash Fault Tolerant, 崩溃容错）

### 2.2 binlog

**binlog 是Mysql Server层的日志， 称归档日志**。Mysql一开始只有binlog，不支持崩溃安全，后来InnoDB使用redolog实现崩溃安全。

binlog与redolog的区别：
1. redolog是InnoDB特有；binlog为server层实现，所有引擎均可使用
2. redolog是物理逻辑，记录**“在某个数据页上做了什么修改”**；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1”
3. redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 

### 2.3 考虑redolog和binlog的更新语句流程

1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。
3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。
4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。

![mysql45-02-2](/images/mysql45-02-2.png)

- 注意：写redolog拆分成两步（两阶段提交）：实际写并标记prepare -> 执行器提交事务后引擎将状态更新为commit

### 2.4 两阶段提交

如果redolog不使用**两阶段提交**， 那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。
- 误操作之后需要根据日志恢复Mysql
- 扩容（常见做法是全量备份加应用binlog）也需要。扩容时如果不是两阶段提交，会导致线上主从数据库不一致

```
由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。

仍然用前面的 update 语句来做例子。假设当前 ID=2 的行，字段 c 的值是 0，再假设 执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，会出现什么情况呢？

1. 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。

2. 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。
```

redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。

### 建议

- redo log 用于保证 crash-safe 能力。`innodb_flush_log_at_trx_commit` 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。

- `sync_binlog` 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。

### 提问

1. 如果要实现 “MySQL 可以恢复到半个月内任意一秒的状态”， 怎么操作？

```
A: 

前面我们说过了，binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的 DBA 承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有 binlog，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。

当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：

    1. 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；
    2. 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。

这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。
```

2. 定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？

```
A:

一天一备 “最长恢复时间”更短。 
一天一备情况下最坏情况下需要应用一天的binlog。 而一周一备最坏情况下需要应用一周的binlog，“RTO”（恢复目标时间）更长

而 更频繁的全量备份 也意味着 需要更多存储空间
```

## 3. 事务隔离：为什么你改了我还看不见？

**事务就是要保证一组数据库操作，要么全部成功，要么全部失败。** 在 MySQL 中，事务支持是*在引擎层实现*的。你现在知道，MySQL 是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如 MySQL 原生的 MyISAM 引擎就不支持事务，这也是 MyISAM 被 InnoDB 取代的重要原因之一。

### 3.1 隔离性与隔离级别

- 事务 ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、**隔离性**、持久性）
    - 上一节提到的redolog和binlog保证了事务的持久性
- 多个事务同时执行的时候，就可能出现**脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）**的问题，为了解决这些问题，就有了“隔离级别”的概念。

SQL 标准的事务隔离级别：

- **读未提交**（read uncommitted）。 是指，一个事务还没提交时，它做的变更就能被别的事务看到。
- **读提交**（read committed）。 是指，一个事务提交之后，它做的变更才会被其他事务看到。
- **可重复读**repeatable read）。 是指，一个事务执行过程中看到的数据，总是**跟这个事务在启动时看到的数据是一致的**。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
- **串行化**（serializable ）。顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当**出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成**，才能继续执行

其中“读提交”和“可重复读”比较难理解，所以我用一个例子说明这几种隔离级别。假设数据表 T 中只有一列，其中一行的值为 1，下面是按照时间顺序执行两个事务的行为。

```sql
mysql> create table T(c int) engine=InnoDB;
insert into T(c) values(1);
```

![mysql45-03-1](/images/mysql45-03-1.png)

- 若隔离级别是“读未提交”， 则 V1 的值就是 2。这时候事务 B 虽然还没有提交，但是结果已经被 A 看到了。因此，V2、V3 也都是 2。
- 若隔离级别是“读提交”，则 V1 是 1，V2 的值是 2。事务 B 的更新在提交后才能被 A 看到。所以， V3 的值也是 2。
- 若隔离级别是“可重复读”，则 V1、V2 是 1，V3 是 2。之所以 V2 还是 1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。
- 若隔离级别是“串行化”，则在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行。所以从 A 的角度看， V1、V2 值是 1，V3 的值是 2。

在实现上，数据库里面会创建一个**视图**，访问的时候以视图的逻辑结果为准。**在“可重复读”隔离级别下，这个视图是在事务启动时创建的**，整个事务存在期间都用这个视图。**在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的**。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。

配置事务隔离级别的方法：配置启动参数 `transaction-isolation`

哪个隔离级别都有它自己的使用场景，你要根据自己的业务情况来定。

### 3.2 事务隔离的实现

读未提交和串行化实现没啥好讲的，一个是不作为，一个是加锁。

读未提交和可重复读都依赖于**视图**这个概念和**undo log**(回滚日志)

下面讲可重复读。

在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。

![mysql45-03-2](/images/mysql45-03-2.png)

当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。

**回滚日志何时删除？**
- 在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。

**为什么尽量不使用长事务？**
- 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。
- 在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。（这与mysql表数据存储方式有关）
- 除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库

### 3.3 事务的启动方式

MySQL 的事务启动方式有以下几种：
- 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。
- set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。
    - 些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。

建议：
- 总是使用 set autocommit=1, 通过显式语句的方式来启动事务  
- 对于需要频繁使用事务的业务，如果顾虑每次开启事务主动执行“begin”带来的交互次数，可以使用 `commit work and chain` 语法。
    - 在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。

长事务的查询：
- 可以在 information_schema 库的 innodb_trx 这个表（里边放的是还没提交的事务）中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。
```sql
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60
```

### 提问

系统里面应该避免长事务，如果你是业务开发负责人同时也是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？

```
A: 
这个问题，我们可以从应用开发端和数据库端来看。

首先，从应用开发端来看：
1. 确认是否使用了 set autocommit=0。这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。
2. 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。我见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。这种只读事务可以去掉。
3. 业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例）

其次，从数据库端来看：
1. 监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill；
2. Percona 的 pt-kill 这个工具不错，推荐使用；
3. 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题；
4. 如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。
```

## 4. 深入浅出索引（上）

- 索引是数据库系统里面最重要的概念之一
- 索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。

### 4.1 索引的常见模型

- 哈希表：
    - 哈希表这种结构适用于只有等值查询的场景，比如 Memcached 及其他一些 NoSQL 引擎。
- 有序数组：
    - 有序数组在等值查询和范围查询场景中的性能就都非常优秀
    - 更新数据时维护成本太高，只适合静态存储引擎
- 搜索树
    - 各项操作性能都较高
    - 二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。
    - 为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。
如今，跳表、B+树、LSM树等用的比较多

### 4.2 InnoDB索引模型

- 在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为**索引组织表**。
- **B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数**。
- InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。
- 每一个索引在 InnoDB 里面对应一棵 B+ 树。
- 根据叶子节点的内容，索引类型分为**主键索引**和**非主键索引**。
    - 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为**聚簇索引**（clustered index）。
    - 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为**二级索引**（secondary index）。
    - 普通索引查询时 须先在普通索引数中查询到对应主键，再根据主键到主键索引树查询到数据行，这个过程叫**回表**
    - 也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。

### 4.3 B+树索引维护

- B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护
- 插入数据时，若数据页已满，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为**页分裂**。
    - 页分裂导致插入性能下降
    - 页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。
- 当相邻两个页由于删除了数据，利用率很低之后，会将数据页做**页合并**。合并的过程，可以认为是分裂过程的逆过程。

### 4.4 讨论：什么场景下适合使用自增主键

- **自增主键**是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT
    - 插入新记录时可以不指定ID，系统会获取当前 ID 最大值加 1 作为下一条记录的 ID 值。
    - 性能上：
        - 自增主键的插入数据模式，正符合了**递增插入**的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也**不会触发叶子节点的分裂**。
        - 而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。
    - 存储空间：
        - 主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。
        - 举例：假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，使用身份证号做主键，那么每个二级索引的叶子节点占用约 20 个字节，而如果用自增整型做主键，则只要 4 个字节，如果是长整型（bigint）则是 8 个字节。
    - 从性能和存储空间方面考量，**自增主键*往往*是更合理的选择**。    
    - *对于典型的KV场景（只有一个索引；该索引必须是唯一索引），由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。*

### 提问

为表T重建索引时，会遇到两种：
1. 重建普通索引k
```sql
alter table T drop index k;
alter table T add index(k);
```
2. 重建主键索引id
```sql
alter table T drop primary key;
alter table T add primary key(id);
```
对于上面这两个重建索引的作法，说出你的理解。如果有不合适的，为什么，更好的方法是什么？

```
A:
重建索引 k 的做法是合理的，可以达到省空间的目的。
但是，重建主键的过程不合理。
不论是删除主键还是创建主键，都会将整个表重建。
所以连着执行这两个语句的话，第一个语句就白做了。
这两个语句，你可以用这个语句代替 ： 
alter table T engine=InnoDB。

ps: 
为什么要重建索引?
    索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。
```

## 5. 深入浅出索引（下）

### 5.1 一条查询语句的执行流程（索引树中）

考虑表（主键ID，普通索引k）：

![mysql45-05-1](/images/mysql45-05-1.png)

执行`select * from T where k between 3 and 5`， 流程为：

1. 在 k 索引树上找到 k=3 的记录，取得 ID = 300；
2. 再到 ID 索引树查到 ID=300 对应的 R3；
3. 在 k 索引树取下一个值 k=5，取得 ID=500；
4. 再回到 ID 索引树查到 ID=500 对应的 R4；
5. 在 k 索引树取下一个值 k=6，不满足条件，循环结束。

共在k树查了3次，回表了2次。

在这个例子中，由于查询结果所需要的数据只在主键索引上有，所以不得不回表。那么，有没有可能经过索引优化，避免回表过程呢？

### 5.2 覆盖索引

- 如果执行的语句是 `select ID from T where k between 3 and 5`，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为**覆盖索引**。
- **由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段**。

给出执行`select ID from T where k between 3 and 5`的流程：
1. 在 k 索引树上找到 k=3 的记录，取得 ID = 300；
2. 在 k 索引树取下一个值 k=5，取得 ID=500；
3. 在 k 索引树取下一个值 k=6，不满足条件，循环结束。

可见在k索引树读了三次，而不必再回表。
    - 注意：虽然在k树读了三次，但对于 MySQL 的 Server 层来说，它就是找引擎拿到了两条记录，因此 MySQL 认为扫描行数是 2

讨论：在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？
- 如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。

### 5.3 最左前缀原则

**B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。**

不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。

利用索引的“最左前缀”，有什么好处？
- 假设有name, age两个字段，但是有三个高频查询（不高频不一定需要建立索引），一个是按name查；一个是按age查；一个是按name和age查（先查name再查age）
- 不考虑“最左前缀”，就需要建立三个索引：index(name)、index(age)、index(name, age)
- 考虑“最左前缀”，由于联合索引index(name, age)是从左向右匹配的，所以可以替代掉index(name)，因此只需要建立两个索引：index(age)、index(name, age)。 
- 联合索引中字段的顺序也要考虑。如果只有两个高频请求（一个是按name查；一个是按name和age查（先查name再查age））。联合索引可以是index(name, age)，也可以是index(age, name)，但选前者的话，可以不用建立索引index(name)，自然是更好的。**在建立联合索引的时候，如何安排索引内的字段顺序？ 第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。**
- 如果继续考虑上面的那三个高频请求，那么两种索引组合：`[index(name)，index(name,age)]`和`[index(age)，index(age,name)]`哪个更好呢？**调整顺序无法减少索引数时，选择索引空间占用更小的方案**，由于name字段比age字段大，选后者

### 5.4 索引下推

MySQL 5.6 引入**索引下推优化**（index condition pushdown)， 可以**在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录**，减少回表次数。

在 MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值。

举例：
- 联合索引index(name, age)，现在要查*姓张的、10岁的*所有结果。
    - 没有索引下推优化时，只能按“最左前缀”原则在索引树上找到所有*姓张的*主键ID，再回表去判断这些记录哪些是*10岁的*。
    - 有索引下推优化时，在index(name,age)索引树时就会直接对*姓张的*索引节点检查是否*10岁的*。减少了回表次数

### 5.5 总结

在满足语句需求的情况下， 尽量少地访问资源是数据库设计的重要原则之一。我们在使用数据库的时候，尤其是在设计表结构时，也要以减少资源消耗作为目标。

### 提问

实际上主键索引也是可以使用多个字段的。DBA 小吕在入职新公司的时候，就发现自己接手维护的库里面，有这么一个表，表结构定义类似这样的：

```sql
CREATE TABLE `geek` (
  `a` int(11) NOT NULL,
  `b` int(11) NOT NULL,
  `c` int(11) NOT NULL,
  `d` int(11) NOT NULL,
  PRIMARY KEY (`a`,`b`),
  KEY `c` (`c`),
  KEY `ca` (`c`,`a`),
  KEY `cb` (`c`,`b`)
) ENGINE=InnoDB;
```
公司的同事告诉他说，由于历史原因，这个表需要 a、b 做联合主键，这个小吕理解了。

但是，既然主键包含了 a、b 这两个字段，那意味着单独在字段 c 上创建一个索引，就已经包含了三个字段了呀，为什么要创建“ca”“cb”这两个索引？

同事告诉他，是因为他们的业务里面有这样的两种语句：

```sql
select * from geek where c=N order by a limit 1;
select * from geek where c=N order by b limit 1;
```

给你的问题是，这位同事的解释对吗，为了这两个查询模式，这两个索引是否都是必须的？为什么呢？

```
A:
不对。
联合主键index(a,b)的比较顺序是：a,b
联合索引index(c,a)的比较顺序是：c,a,(a,b) 等价于 c,a,b 等价于 c,(a,b) 因此不需要该联合索引，使用索引index(c)就好
联合索引index(c,b)的比较顺序是：c,b,(a,b) 等价于 c,b,a 需要保留

```

## 6. 全局锁和表锁 ：给表加个字段怎么有这么多阻碍？

数据库锁设计的初衷是处理并发问题。作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。

根据加锁的范围，MySQL 里面的锁大致可以分成**全局锁、表级锁和行锁**三类

### 6.1 全局锁

- 含义：全局锁就是对整个数据库实例加锁。
- 方法：MySQL 提供了一个加全局读锁的方法，命令是 `Flush tables with read lock` (**FTWRL**)。
- 效果：当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。
- 使用场景：
    - **全库逻辑备份**。也就是把整库每个表都 select 出来存成文本。

讨论：全局逻辑备份的方式与优缺点：
- 使用FTWRL全局读锁，然后整库备份
    - 问题：数据库一般都是主从架构。在主库上执行全局备份，那么备份期间系统无法执行更新，业务就得停住；在从库执行备份，那么备份期间从库不能执行主库同步过来的binlog，会导致主从延迟
    - 必要性：不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。
    - 使用建议：在不能使用`single-transaction`情况下，使用从库加全局锁而后备份
- **在可重复读隔离级别下开启一个事务**，在这个事务中做全局备份
    - 在可重复读隔离级别下开启一个事务，能够拿到一致性视图
    - 官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。
    - 局限性：**single-transaction 方法只适用于所有的表使用事务引擎的库。**
- 使用 `set global readonly=true` 的方式保证全库只读
    - 不建议使用该策略，因为：
        - 一是，在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。
        - 二是，在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。


业务的更新包括：
- 增删改数据（DML)
- 加字段等修改表结构的操作（DDL）

加全局读锁FTWRL之后，无法DDL

即使没有加FTWRL，DDL还要看是否有表级锁

### 6.2 表级锁

Mysql表级锁：一种是**表锁**，一种是**元数据锁（meta data lock，MDL)**。

**表锁**
- 语法： `lock tables … read/write`。 与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。
- 注意： lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。
    - 例如线程A中执行 `lock tables t1 read, t2 write;`， 那么在unlock之前，线程A只能对t1进行读操作，对t2可以读写
- 对于支持行锁的引擎，一般不用表锁控制并发

**元数据锁MDL**
- MDL 不需要显式使用，在访问一个表的时候会被自动加上。
- 在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。
- MDL 的作用是，保证读写的正确性。
- 注意：事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。（换句话说，MDL 会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。）
    - 考虑对一张表执行一个事务，事务中操作的先后顺序是：(1)启动事务begin，查某记录行 (2)查某记录行 (3)表字段变更 (4)查某记录行。（注意事务还没提交）。
    - 这种情况下，(1)步时加了MDL读锁，但是事务未提交的话MDL读锁一直存在，接下来(2)可以执行，(3)需要写锁因此被阻塞直到事务提交，要命的是(3)之后的所有原本只需要读锁的操作都会被阻塞，
        - 导致该表完全不可读写
        - 如果查询频繁且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。

**讨论：如何安全地给小表加字段？**
- 对大表加字段一般非常小心；小表可能因为粗心大意导致上面的问题。
- 首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要**做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。**        
- 但考虑一下这个场景。如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，你该怎么做呢？这时候 kill 可能未必管用，因为新的请求马上就来了。比较理想的机制是，**在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。**之后开发人员或者 DBA 再通过重试命令重复这个过程。

### 提问

备份一般都会在备库上执行，你在用–single-transaction 方法做逻辑备份的过程中，如果主库上的一个小表做了一个 DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？

```
A:

假设这个 DDL 是针对表 t1 的， 这里我把备份过程中几个关键的语句列出来：

Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;
Q2:START TRANSACTION  WITH CONSISTENT SNAPSHOT；
/* other tables */
Q3:SAVEPOINT sp;
/* 时刻 1 */
Q4:show create table `t1`;
/* 时刻 2 */
Q5:SELECT * FROM `t1`;
/* 时刻 3 */
Q6:ROLLBACK TO SAVEPOINT sp;
/* 时刻 4 */
/* other tables */


在备份开始的时候，为了确保 RR（可重复读）隔离级别，再设置一次 RR 隔离级别 (Q1);

启动事务，这里用 WITH CONSISTENT SNAPSHOT 确保这个语句执行完就可以得到一个一致性视图（Q2)；

设置一个保存点，这个很重要（Q3）；

show create 是为了拿到表结构 (Q4)，然后正式导数据 （Q5），回滚到 SAVEPOINT sp，在这里的作用是释放 t1 的 MDL 锁 （Q6）。当然这部分属于“超纲”，上文正文里面都没提到。

DDL 从主库传过来的时间按照效果不同，我打了四个时刻。题目设定为小表，我们假定到达后，如果开始执行，则很快能够执行完成。

参考答案如下：

1. 如果在 Q4 语句执行之前到达，现象：没有影响，备份拿到的是 DDL 后的表结构。
2. 如果在“时刻 2”到达，则表结构被改过，Q5 执行的时候，报 Table definition has changed, please retry transaction，现象：mysqldump 终止；
3. 如果在“时刻 2”和“时刻 3”之间到达，mysqldump 占着 t1 的 MDL 读锁，binlog 被阻塞，现象：主从延迟，直到 Q6 执行完成。
4. 从“时刻 4”开始，mysqldump 释放了 MDL 读锁，现象：没有影响，备份拿到的是 DDL 前的表结构。
```

## 7. 行锁功过：怎么减少行锁对性能的影响？

MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。**InnoDB 是支持行锁的**，这也是 MyISAM 被 InnoDB 替代的重要原因之一。

下面均介绍 **如何通过减少锁冲突**来提升业务并发度。

### 7.1 两阶段锁

在 InnoDB 事务中，行锁是**在需要的时候才加上**的，但并不是不需要了就立刻释放，而是要**等到事务结束时才释放**。这个就是**两阶段锁协议**。
    - 如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。

### 7.2 死锁与死锁检测

一个死锁的例子：

![mysql45-07-1](/images/mysql45-07-1.png)

事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略：
- 一种策略是，**直接进入等待，直到超时**。这个超时时间可以通过参数 `innodb_lock_wait_timeout` (默认50s) 来设置。
    - 在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。
    - 但是，我们又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。
- 另一种策略是，**发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务**，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 on，表示开启这个逻辑。
    - 正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。
    - 每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。时间复杂度O(n)。对于热点行更新来说的话，假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。这导致CPU 利用率很高，但是每秒却执行不了几个事务。

**讨论：如何解决由这种热点行更新导致的性能问题呢？**
- 问题的症结在于，死锁检测要耗费大量的 CPU 资源。
- 一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以**临时把死锁检测关掉**。  
    - 但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。
- 另一个思路是**控制并发度**（访问相同资源的并发事务量）。   
    - 这个**并发控制要做在数据库服务端**。如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改 MySQL 源码的人，也可以做在 MySQL 里面。基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了。
    - 客户端做并发控制没用，因为客户端可能很多，对于数据库服务端来讲，并发依然很高
- 在不能控制并发度的情况下，**可以考虑通过将一行改成逻辑上的多行来减少锁冲突**。
    - 以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成 0 的时候，代码要有特殊处理。

### 提问

如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：
1. 第一种，直接执行 delete from T limit 10000;
2. 第二种，在一个连接中循环执行 20 次 delete from T limit 500;
3. 第三种，在 20 个连接中同时执行 delete from T limit 500。
你会选择哪一种方法呢？为什么呢？

```
A:
第二种方式是相对较好的。
第一种方式（即：直接执行 delete from T limit 10000）里面，单个语句占用时间长，锁的时间也比较长；而且大事务还会导致主从延迟。
第三种方式（即：在 20 个连接中同时执行 delete from T limit 500），会人为造成锁冲突。
```

## 8. 事务到底是隔离的还是不隔离的？

前面讲到：
- 讲事务隔离时，提到如果是可重复读隔离级别，事务 T 启动的时候会创建一个视图 read-view，之后事务 T 执行期间，即使有其他事务修改了数据，事务 T 看到的仍然跟在启动时看到的一样。也就是说，一个在可重复读隔离级别下执行的事务，好像与世无争，不受外界影响。
- 讲行锁时，提到一个事务要更新一行，如果刚好有另外一个事务拥有这一行的行锁，它又不能这么超然了，会被锁住，进入等待状态。问题是，既然进入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值又是什么呢？

考虑表：
```sql
mysql> CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `k` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;
insert into t(id, k) values(1,1),(2,2);
```

按下图执行三个事务（从上至下为时间先后顺序）

![mysql45-08-1](/images/mysql45-08-1.png)

执行结果：事务 B 查到的 k 的值是 3，而事务 A 查到的 k 的值是 1

分析：

- 事务 C 没有显式地使用 begin/commit，表示这个 update 语句本身就是一个事务，语句完成的时候会自动提交。（因此在事务C的视角，k更新为2）
- 事务 B 在更新了行之后查询 ; （可重复读级别下，由于B在C提交之后进行更新，因此B在k=2的基础上将之更新为3，所以B中得到k=3）
- 事务 A 在一个只读事务中查询，并且时间顺序上是在事务 B 的查询之后。（可重复读级别，只读事务A的视角中看到的k停留在一开始的状态）

从这个过程可以看出来：查询与更新不同。事务隔离级别是针对读。

### 8.1 事务的启动时机与方式

- 第三章提到：事务启动可以通过显式`begin/start transaction`，也可以`set autocommit=0`这样执行第一个sql语句，事务就启动了并且不会自动提交。
- 这里补充：`begin/start transaction` 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 `start transaction with consistent snapshot` 这个命令。
    - 第一种启动方式，一致性视图是在第执行第一个快照读语句时创建的；
    - 第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的。
- 我们的例子中如果没有特别说明，都是默认 autocommit=1。

### 8.2 视图

在 MySQL 里，有两个“视图”的概念：
- 一个是 `view`。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 `create view … `，而它的查询方法与表一样。
- 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 `consistent read view`，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。

它没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”。

### 8.3 “快照”在MVCC里如何工作

“快照”并不是把当前状态的数据拷贝一份出来，这样做成本太高。

InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。

而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。

也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。

如图 2 所示，就是一个记录被多个事务连续更新后的状态。
![mysql45-08-2](/images/mysql45-08-2.png)

你可能会问，前面的文章不是说，语句更新会生成 undo log（回滚日志）吗？那么，undo log 在哪呢？

实际上，图 2 中的三个虚线箭头，就是 undo log；

而 V1、V2、V3 并不是物理上真实存在的，而是**每次需要的时候根据当前版本和 undo log 计算出来**的。比如，需要 V2 的时候，就是通过 V4 依次执行 U3、U2 算出来。

按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。

因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。

当然，如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的。

在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前**正在“活跃”的所有事务 ID**。**“活跃”指的就是，启动了但还没提交。**

数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。

这个视图数组和高水位，就组成了当前事务的**一致性视图**（read-view）。

而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。

这个视图数组把所有的 row trx_id 分成了几种不同的情况。

![mysql45-08-3](/images/mysql45-08-3.png)

这样，对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能：
1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；
2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；
3. 如果落在黄色部分，那就包括两种情况
    - a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；
    - b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。

备注：这里是这么理解：
- 图里边的低水位、高水位、事务ID数组，针对的都是整个系统里边的事务ID，对于用户来讲，可以在information_schema 库的 innodb_trx 这个表查看到，（当然数据库系统可能不需要读这张表，也许维护在内存，但这不是重点）
- 而 row trx_id 是数据行上的版本号，这个版本号就是某事务A访问该数据行时所产生的版本号，数值上是相等的。
- 对于上面的“如果落在黄色部分，...”是这么个意思：先直接通过比较高低水位，来快速判断（这笔直接上来就查数组要好得多），发现通过高低水位还不能判定，再遍历数组，看数据行的某个版本（row trx_id）是否是一个以已提交的事务的版本，还是未提交的

总结：**InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。**

接下来，我们继续看一下图 1 中的三个事务，分析下事务 A 的语句返回的结果，为什么是 k=1。

这里，我们不妨做如下假设：
- 事务 A 开始前，系统里面只有一个活跃事务 ID 是 99；
- 事务 A、B、C 的版本号分别是 100、101、102，且当前系统里只有这四个事务；
- 三个事务开始前，(1,1）这一行数据的 row trx_id 是 90。

这样，事务 A 的视图数组就是 [99,100], 事务 B 的视图数组是 [99,100,101], 事务 C 的视图数组是 [99,100,101,102]。

为了简化分析，我先把其他干扰语句去掉，只画出跟事务 A 查询逻辑有关的操作：
![mysql45-08-4](/images/mysql45-08-4.png)
按照前面分析的`row trx_id`可见性判断的规则，可以得到A确实读到k=1。

事务A不论在事务期间何时查询，看到的这行数据都是一致的，这称为**一致性读**

数据版本可见性规则描述：

**一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：**
1. 版本未提交，不可见；
2. 版本已提交，但是是在视图创建后提交的，不可见；
3. 版本已提交，而且是在视图创建前提交的，可见。

### 8.4 更新逻辑

上面的例子中，事务 B 的 update 语句，如果按照一致性读，好像结果不对哦？

这是因为：

**更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。**
    - 这句话的意思就是update语句是当前读，而一般的不加锁的select是一致性读
    - 这里我们提到了一个概念，叫作当前读。其实，除了 update 语句外，**select 语句如果加锁，也是当前读**。
        - 所以，如果把事务 A 的查询语句 select * from t where id=1 修改一下，加上 lock in share mode 或 for update，也都可以读到版本号是 101 的数据，返回的 k 的值是 3。下面这两个 select 语句，就是分别加了读锁（S 锁，共享锁）和写锁（X 锁，排他锁）。
        ```sql
        mysql> select k from t where id=1 lock in share mode;
        mysql> select k from t where id=1 for update;
        ```

讨论：加入事务C不是马上提交，而变成如下：

![mysql45-08-5](/images/mysql45-08-5.png)

根据“两阶段锁协议”，事务C没提交，那么事务C在(1,2)这个版本的写锁还没释放，事务B的update语句需要获取写锁，因此阻塞等待。

### 8.5 事务可重复读能力是怎么实现的

根据前面，总结如下：

- **可重复读的核心就是一致性读（consistent read）**；
- 而事务**更新数据的时候，只能用当前读**。
- 如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

### 8.6 读提交

而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：

- 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
- **在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图**。

回到前面的例子，现在设隔离级别是读提交，并且start transaction with consistent snapshot; ”的意思是从这个语句开始，创建一个持续整个事务的一致性快照。所以，在读提交隔离级别下，这个用法就没意义了，等效于普通的 start transaction。

在读提交隔离级别下，事务 A 和事务 B 的查询语句查到的 k，分别应该是多少呢？

A k=2 B k=3

### 8.7 总结

InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。
- 对于可重复读，查询只承认在事务启动前就已经提交完成的数据；
- 对于读提交，查询只承认在语句启动前就已经提交完成的数据；

而**当前读，总是读取已经提交完成的最新版本**。

你也可以想一下，为什么表结构不支持“可重复读”？这是因为表结构没有对应的行数据，也没有 row trx_id，因此只能遵循当前读的逻辑。
    - 当然，MySQL 8.0 已经可以把表结构放在 InnoDB 字典里了，也许以后会支持表结构的可重复读。

名词汇总：
- 一致性读
- 一致性视图（read view）
- 当前读（current read）
- 事务ID（transaction id）
- 数据行版本号 （row trx_id）
- 多版本并发控制 （MVCC）

## 9. 普通索引和唯一索引，应该怎么选择？

- 查询操作区别微乎其微
    - 对于查询，唯一索引可以查找目标后直接返回，普通索引需要再往后查一个，判断不是目标才停止
    - 由于数据加载到内存(buffer pool)中是整页(16KB)加载，大多数情况下普通索引最后要查的下一个也在内存中，因此均摊时间依然是一次内存查询判断
- 更新操作
    - 若更新的数据行原本在内存中，则区别微乎其微
        - 都是更新内存数据行，写redolog
    - 若不在内存中，则普通索引可以利用change buffer优化随机读磁盘
        - 如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。
        - 需要说明的是，虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。
        - 将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。
        - 显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。
        - 普通索引可以直接将更新操作写到change buffer，等到下次需要查询时再从磁盘读数据行并应用这些change，得到最新状态，这称为merge，更新后的数据需要写redolog。
        - 由于唯一索引一定需要先从磁盘读数据行，来校验唯一性，所以无法使用change buffer优化
        - 将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。

### 9.1 change buffer使用场景

因为 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。

- 生效于普通索引
- 适用于写多读少场景
    - 读多写少的话，刚写完change buffer可能立马又读，然后发生了merge，不仅不能提升性能，还浪费内存
    - 常见场景：账单类、日志类

### 9.2 索引选择与实践

- 普通索引与唯一索引只在更新性能有区别，尽量选择普通索引
- 如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。
    - 普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。
    - 特别地，在使用机械硬盘时，change buffer 这个机制的收效是非常显著的。所以，当你有一个类似“历史数据”的库，并且出于成本考虑用的是机械硬盘时，那你应该特别关注这些表里的索引，尽量使用普通索引，然后把 change buffer 尽量开大，以确保这个“历史数据”表的数据写入速度。

- 补充：业务正确性优先，如果业务要求数据库做唯一性约束，那么只能使用唯一索引。    

### 9.3 change buffer与redo log

假设现在有更新语句要更新page1(在buffer pool中)和page2(不在buffer pool)

那么更新流程是：
1. page1在内存，直接更新内存
2. page2不在内存，直接将更新操作写到change buffer
3. 将动作1,2写到redolog
    - 这就是change buffer的持久化，也是写到redolog的

这次更新总体来说写了两处内存，一处磁盘（redolog日志）

紧接着有读请求，假设page1还在内存。
1. 读page1直接从buffer pool中读取
2. 读page2，将之从磁盘读到内存，应用change buffer中的操作日志，生成一个正确的版本并返回结果

可以看到，直到需要读 Page 2 的时候，这个数据页才会被读入内存。

所以，如果要简单地对比这两个机制在提升更新性能上的收益的话，**redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。**

### 提问

change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢？change buffer 丢失可不是小事儿，再从磁盘读入数据可就没有了 merge 过程，就等于是数据丢失了。会不会出现这种情况呢？

```
A:

答案是不会丢失。虽然是只更新内存，但是在事务提交的时候，我们把 change buffer 的操作也记录到 redo log 里了，所以崩溃恢复的时候，change buffer 也能找回来。

ps: merge 的过程是否会把数据直接写回磁盘?
merge 的执行流程是这样的：

1. 从磁盘读入数据页到内存（老版本的数据页）；
2. 从 change buffer 里找出这个数据页的 change buffer 记录 (可能有多个），依次应用，得到新版数据页；
3. 写 redo log。这个 redo log 包含了数据的变更和 change buffer 的变更。

到这里 merge 过程就结束了。这时候，数据页和内存中 change buffer 对应的磁盘位置都还没有修改，属于脏页，之后各自刷回自己的物理数据，就是另外一个过程了。

也就是说，这些操作的持久化，都是依赖于redolog的，写到redolog就代表持久化了，redolog自己再异步更新到索引树中
```

## 10. MySQL为什么有时候会选错索引？

在 MySQL 中一张表其实是可以支持多个索引的。但是，你写 SQL 语句的时候，并没有主动指定使用哪个索引。也就是说，使用哪个索引是由 MySQL 来确定的。

当发现按索引查询很慢时，可以这样做：
- `explain YOURSQL`，查看`key`列是否是预期的索引列，不是则说明选错索引，可以使用`force index(YOURINDEXKEY)`来强制使用指定索引
- 查看慢查询日志（slow log）分析是哪些语句执行比较慢

### 10.1 优化器的逻辑

- 选择索引是优化器的工作。
- 优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句
- 在数据库里面，扫**描行数是影响执行代价的因素之一**。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。
    - 当然，扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。
    - **扫描行数的判断有可能导致优化器选错索引**

**扫描行数是怎么判断的？**
- MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。
- 这个统计信息就是索引的“区分度”。
    - 显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上**不同的值的个数**，我们称之为“基数”（cardinality）。也就是说，这个基数**越大，索引的区分度越好**。
    - `show index from t`查看表t的所有索引信息，其中就有各个索引的“基数”

**MySQL 是怎样得到索引的基数的呢？**
- 采样统计
    - 把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太高了，所以只能选择“采样统计”
    - 采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。
    - 而数据表是会持续更新的，索引统计信息也不会固定不变。所以，当变更的数据行数超过 1/M 的时候，会自动触发重新做一次索引统计。
    - 在 MySQL 中，有两种存储索引统计的方式，可以通过设置参数 innodb_stats_persistent 的值来选择：
        - 设置为 on 的时候，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10。
        - 设置为 off 的时候，表示统计信息只存储在内存中。这时，默认的 N 是 8，M 是 16。
    - 由于是采样统计，所以不管 N 是 20 还是 8，这个基数都是很容易不准的。

优化器选择索引时除了估计的扫描行数外，还会考虑是不是主键索引：
- 在非主键索引扫描时，还需要回表，这是额外开销
- 因此，当主键索引扫描行数比非主键索引扫描行数多，但是没有多出数量级，这时，优化器可能选择主键索引。

当我们定位问题，发现是由于索引扫描行数估计不对导致选错索引，那么
    - 手动重新统计索引信息 `analyze table t`
    - 在实践中，如果你发现 explain 的结果预估的 rows 值跟实际情况差距比较大，可以采用这个方法来处理。

### 10.2 索引选择异常和处理

- `force index(YOURINDEX)`强制指定
    - 但要注意，这会导致索引改名之后，语句也得改；而且迁移到别的数据库时可能语法不兼容
- **考虑修改语句，引导 MySQL 使用我们期望的索引**
- **在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。**

## 11. 怎么给字符串字段加索引？

创建完整索引
    - 可能比较占用空间

### 11.1 合适的前缀索引

MySQL 是支持前缀索引的，也就是说，你可以定义字符串的一部分作为索引。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。

**使用前缀索引，定义好合适的长度，就可以做到既节省空间，又不用额外增加太多的查询成本。**

当要给字符串创建前缀索引时，**有什么方法能够确定我应该使用多长的前缀呢？**
- 实际上，我们在建立索引时关注的是区分度，区分度越高越好。因为区分度越高，意味着重复的键值越少。因此，我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。
    1. 首先，你可以使用`select count(distinct YOURINDEXKEY) as L from YOURTABLE;`，算出这个列上有多少个不同的值
    2. 然后，依次选取不同长度的前缀来看这个值。当然，使用前缀索引很可能会损失区分度，所以你需要预先设定一个可以接受的损失比例，比如 5%。然后，在返回的 L4~L7 中，找出不小于 L * 95% 的值，假设这里 L6、L7 都满足，你就可以选择前缀长度为 6。
    ```sql
    mysql> select 
    count(distinct left(YOURINDEXKEY,4)）as L4,
    count(distinct left(YOURINDEXKEY,5)）as L5,
    count(distinct left(YOURINDEXKEY,6)）as L6,
    count(distinct left(YOURINDEXKEY,7)）as L7,
    from YOURTABLE;
    ```

**前缀索引存在的问题：**
- 可能会增加扫描行数，这会影响到性能
- **使用前缀索引就用不上覆盖索引对查询性能的优化**

### 11.2 给字符串字段加索引的其他方式

对于类似于邮箱这样的字段来说，使用前缀索引的效果可能还不错。但是，遇到前缀的区分度不够好的情况时，我们要怎么办呢？

1. **倒序存储**
    - 例如某县的身份信息表，想给身份证号加索引，前6位为地址码，直接使用长度为6的前缀索引，区分度几乎为0. 这种情况下可以考虑将身份证号倒序存储
2. **使用hash字段**
    - 虽然原字段前缀区分度不高，但哈希后区分度就高了
    - 由于哈希碰撞可能发生，所以查询语句 where 部分要判断 身份证号 的值是否精确相同。

这两种方法的比较：
- **都不支持范围查询**
- 倒序存储不会消耗额外空间，哈希字段方法需要增加字段。但是要注意倒序存储的前缀索引长度要求可能比哈希字段要更长，所以说不好谁空间占用更高
- 在 CPU 消耗方面，倒序方式每次写和读的时候，都需要额外调用一次 reverse 函数，而 hash 字段的方式需要额外调用一次 crc32() 函数。如果只从这两个函数的计算复杂度来看的话，reverse 函数额外消耗的 CPU 资源会更小些。
- 从查询效率上看，使用 hash 字段方式的查询性能相对更稳定一些。因为 crc32 算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。       

其实针对上面身份证号这种情况，可以考虑将一个字段拆成两个字段：地址号 + 后面的部分，后面的部分区分度就高了

### 提问

如果你在维护一个学校的学生信息数据库，学生登录名的统一格式是”学号 @gmail.com", 而学号的规则是：十五位的数字，其中前三位是所在城市编号、第四到第六位是学校编号、第七位到第十位是入学年份、最后五位是顺序编号。

系统登录的时候都需要学生输入登录名和密码，验证正确后才能继续使用系统。就只考虑登录验证这个行为的话，你会怎么设计这个登录名的索引呢？

```
A:

由于这个学号的规则，无论是正向还是反向的前缀索引，重复度都比较高。因为维护的只是一个学校的，因此前面 6 位（其中，前三位是所在城市编号、第四到第六位是学校编号）其实是固定的，邮箱后缀都是 @gamil.com，因此可以只存入学年份加顺序编号，它们的长度是 9 位。

而其实在此基础上，可以用数字类型来存这 9 位数字。比如 201100001，这样只需要占 4 个字节。其实这个就是一种 hash，只是它用了最简单的转换规则：字符串转数字的规则，而刚好我们设定的这个背景，可以保证这个转换后结果的唯一性。
```

## 12. 为什么我的MySQL会“抖”一下？

一条 SQL 语句，正常执行的时候特别快，但是有时也不知道怎么回事，它就会变得特别慢，并且这样的场景很难复现，它不只随机，而且持续时间还很短。看上去，这就像是数据库“抖”了一下。

原因就是：

利用 WAL 技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能。但是，由此也带来了内存脏页的问题。脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些。

### 12.1 刷脏页

InnoDB 在处理更新语句的时候，只做了写日志这一个磁盘操作。这个日志叫作 redo log（重做日志），在更新内存写完 redo log 后，就返回给客户端，本次更新成功。

把内存里的数据写入磁盘的过程，术语就是 flush。在这个 flush 操作执行之前，内存数据页跟磁盘数据页内容不一致

**当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。**

平时执行很快的更新操作，其实就是在写内存和日志，而 MySQL 偶尔“抖”一下的那个瞬间，可能就是在刷脏页（flush）。

**什么情况会引发数据库的 flush 过程呢？**
1. **redo log 写满了**。这时候系统会停止所有更新操作，把 checkpoint 往前推进（推进范围内的redolog需要将对应的内存脏页flush到磁盘），redo log 留出空间可以继续写
2. **系统内存不足**。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。**如果淘汰的是“脏页”，就要先将脏页写到磁盘。**
    - 你一定会说，这时候难道不能直接把内存淘汰掉，下次需要请求的时候，从磁盘读入数据页，然后拿 redo log 出来应用不就行了？这里其实是从性能考虑的。如果刷脏页一定会写盘，就保证了每个数据页有两种状态：
        - 一种是内存里存在，内存里就肯定是正确的结果，直接返回；
        - 另一种是内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。
    - 这样的效率最高。
3. MySQL 认为系统“空闲”的时候。由于Mysql可能有时会请求很高，很快将buffer pool写满，因此，即便是在繁忙的时候，也要见缝插针地找时间，只要有机会就刷一点“脏页”。
4. MySQL 正常关闭时。这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。

**刷脏页对前面两种情形的性能影响**：
1. redolog写满
    - 这种情况是 InnoDB 要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为 0。
2. 系统内存不足，淘汰脏页
    - 这种情况其实是常态
    - InnoDB 用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态：
        - 第一种是，还没有使用的
        - 第二种是，使用了并且是干净页
        - 第三种是，使用了并且是脏页。
    - InnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。
    - 而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。
所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的：
1. 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；
2. 日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。
所以，InnoDB 需要有**控制脏页比例**的机制，来尽量避免上面的这两种情况。

### 12.2 InnoDB刷脏页的控制策略

1. 首先，你要正确地告诉 InnoDB 所在主机的 IO 能力，这样 InnoDB 才能知道需要全力刷脏页的时候，可以刷多快。
    - 将 `innodb_io_capacity` 设置为 磁盘的IOPS
        - 磁盘IOPS通过fio这个工具测试：`fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest `
        - 如果`innodb_io_capacity`过低设置，会导致InnoDB 认为这个系统的能力就这么差，所以刷脏页刷得特别慢，甚至比脏页生成的速度还慢，这样就造成了**脏页累积**，影响了查询和更新性能。
2. 虽然我们现在已经定义了“全力刷脏页”的行为，但平时总不能一直是全力刷吧？毕竟磁盘能力不能只用来刷脏页，还需要服务用户请求。所以接下来，我们就一起看看 InnoDB 怎么控制引擎按照“全力”的百分比来刷脏页。  
    - 如果刷太慢，会出现什么情况？首先是内存脏页太多，其次是 redo log 写满。
    - InnoDB 的刷盘速度就是要参考这两个因素：一个是**脏页比例**，一个是 **redo log 写盘速度**。 
    - InnoDB 会根据这两个因素先单独算出两个数字。
    - 参数 `innodb_max_dirty_pages_pct` 是脏页比例上限，默认值是 75%。InnoDB 会根据当前的脏页比例（假设为 M），算出一个范围在 0 到 100 之间的数字，计算这个数字的伪代码类似这样：
    ```
    F1(M)
    {
        if M>=innodb_max_dirty_pages_pct then
            return 100;
        return 100*M/innodb_max_dirty_pages_pct;
    }
    ```  
    - InnoDB 每次写入的日志都有一个序号，当前写入的序号跟 checkpoint 对应的序号之间的差值，我们假设为 N。InnoDB 会根据这个 N 算出一个范围在 0 到 100 之间的数字，这个计算公式可以记为 F2(N)。F2(N) 算法比较复杂，你只要知道 N 越大，算出来的值越大就好了。
    - **根据上述算得的 F1(M) 和 F2(N) 两个值，取其中较大的值记为 R，之后引擎就可以按照 innodb_io_capacity 定义的能力乘以 R% 来控制刷脏页的速度。** 

现在你知道了，InnoDB 会在后台刷脏页，而刷脏页的过程是要将内存页写入磁盘。所以，无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用 IO 资源并可能影响到了你的更新语句，都可能是造成你从业务端感知到 MySQL“抖”了一下的原因。

要尽量避免这种情况，你就要合理地设置 innodb_io_capacity 的值，并且**平时要多关注脏页比例，不要让它经常接近 75%**。

脏页比例 = `Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total`

接下来，我们再看一个有趣的策略。

一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL 中的一个机制，可能让你的查询会更慢：**在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉**；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。

在 InnoDB 中，`innodb_flush_neighbors` 参数就是用来控制这个行为的，值为 1 的时候会有上述的**连坐机制**，值为 0 时表示不找邻居，自己刷自己的。

找“邻居”这个优化在机械硬盘时代是很有意义的，可以减少很多随机 IO。机械硬盘的随机 IOPS 一般只有几百，相同的逻辑操作减少随机 IO 就意味着系统性能的大幅度提升。

而如果使用的是 SSD 这类 IOPS 比较高的设备的话，我就建议你把 innodb_flush_neighbors 的值设置成 0。因为这时候 IOPS 往往不是瓶颈，而“只刷自己”，就能更快地执行完必要的刷脏页操作，减少 SQL 语句响应时间。

在 MySQL 8.0 中，innodb_flush_neighbors 参数的默认值已经是 0 了。

### 提问

一个内存配置为 128GB、innodb_io_capacity 设置为 20000 的大规格实例，正常会建议你将 redo log 设置成 4 个 1GB 的文件。

但如果你在配置的时候不慎将 redo log 设置成了 1 个 100M 的文件，会发生什么情况呢？又为什么会出现这样的情况呢？

```
A:

每次事务提交都要写 redo log，如果设置太小，很快就会被写满，也就是下面这个图的状态，这个“环”将很快被写满，write pos 一直追着 CP。

这时候系统不得不停止所有更新，去推进 checkpoint。

这时，你看到的现象就是磁盘压力很小，但是数据库出现间歇性的性能下跌。
```

## 13. 为什么表数据删掉一半，表文件大小不变？

这涉及： 数据库表的空间回收

针对 MySQL 中应用最广泛的 InnoDB 引擎展开讨论。

一个 InnoDB 表包含两部分，即：表结构定义和数据。
- 在 MySQL 8.0 版本以前，表结构是存在以.frm 为后缀的文件里。
- 而 MySQL 8.0 版本，则已经允许把表结构定义放在系统数据表中了。

因为表结构定义占用的空间很小，所以我们今天主要讨论的是表数据。

### 13.1 设置表数据如何存储

**表数据既可以存在共享表空间里，也可以是单独的文件**。这个行为是由参数 `innodb_file_per_table` 控制的：

- 这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；
    - 这种做法下，即使`drop table t`删除表t，空间也不会回收
- 这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。
    - 这种做法下，`drop table t`删除表t后，该文件就删除了
    - 从 MySQL 5.6.6 版本开始，它的默认值就是 ON 了。
    - 这也是推荐设置

但是即便是`innodb_file_per_table = ON`， 在删除整个表的时候，可以使用 drop table 命令回收表空间。但是，我们遇到的更多的删除数据的场景是删除某些行，这时就遇到了我们文章开头的问题：表中的数据被删除了，但是表空间却没有被回收。

### 13.2 数据删除流程

- InnoDB用B+树组织，主键索引树叶子节点存数据行（除非数据行太大）。
- B+树每个节点的大小是1个页（16KB）
- B+树删除某个数据行时，只是标记为删除，之后这个位置就可以复用。但是磁盘文件并不会缩小
- 当某个数据页上所有的数据行（记录）都删除了，那么这个数据页就可以复用了
- **数据页的复用跟记录的复用是不同的**。
    - 记录的复用仅限于按照插入规则应该放到这个位置的数据
    - 数据页的复用，在从B+树里摘除后，可以复用到任意位置
- 当相邻两个页的利用率都很低时，会发生页合并，空出来的那个数据页就被标记为可复用
- 如果用delete删除表所有数据呢？
    - 所有的数据页都会被标记为可复用。但是磁盘上，文件不会变小。
- delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。
- **不止是删除数据会造成空洞，插入数据也会**
    - 如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂（页分裂后就产生了空洞）。
- 新索引上的值，可以理解为删除一个旧的值，再插入一个新值。不难理解，这也是会造成空洞的。
- 也就是说，经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。而重建表，就可以达到这样的目的。

### 13.3 重建表

- 思路：
    1. 新建一个与表 A 结构相同的表 B，
    2. 然后按照主键 ID 递增的顺序，把数据一行一行地从表 A 里读出来再插入到表 B 中。
    3. 把表 B 作为临时表，数据从表 A 导入表 B 的操作完成后，用表 B 替换 A。（重建后的表更紧凑）
- 做法： **`alter table A engine=InnoDB`**
- 存在的问题：
    - 花时间最多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要写入到表 A 的话，就会造成数据丢失。因此，在整个 DDL 过程中，表 A 中不能有更新。也就是说，这个 DDL 不是 Online 的。
- MySQL 5.6 版本开始引入的 **Online DDL**，对这个操作流程做了优化    
- 引入了 Online DDL 之后，重建表的流程：
    1. 建立一个临时文件，扫描表 A 主键的所有数据页；
    2. 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中；
    3. **生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中**；（这里是online DDL 的重点）
    4. 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件；
    5. 用临时文件替换表 A 的数据文件。
- DDL 之前是要拿 MDL 写锁的，这样还能叫 Online DDL 吗？
    - alter 语句在启动的时候需要获取 MDL 写锁，但是这个写锁在真正拷贝数据之前就退化成读锁了。
    - 为什么要退化呢？为了实现 Online，MDL 读锁不会阻塞增删改操作。
    - 那为什么不干脆直接解锁呢？为了保护自己，禁止其他线程对这个表同时做 DDL。
    - 而对于一个大表来说，Online DDL 最耗时的过程就是拷贝数据到临时表的过程，这个步骤的执行期间可以接受增删改操作。所以，相对于整个 DDL 过程来说，锁的时间非常短。对业务来说，就可以认为是 Online 的。
    - 需要补充说明的是，上述的这些重建方法都会扫描原表数据和构建临时文件。对于很大的表来说，这个操作是很消耗 IO 和 CPU 资源的。因此，如果是线上服务，你要很小心地控制操作时间。如果想要比较安全的操作的话，我推荐你使用 GitHub 开源的 gh-ost 来做。
- 注意： **在重建表的时候，InnoDB 不会把整张表占满，每个页留了 1/16 给后续的更新用**。也就是说，其实重建表之后不是“最”紧凑的。   

### 13.4 online和inplace
- 前面说的online DDL过程，临时文件是在InnoDB内部创建的，对于Server层来说，并没有把数据挪动到临时表，是一个“原地”操作，这就是“inplace”名称的来源。
- 如果你有一个 1TB 的表，现在磁盘间是 1.2TB，能不能做一个 inplace 的 DDL 呢？
    - 答案是不能。因为，tmp_file 也是要占用临时空间的。
- online DDL的语句`alter table t engine=InnoDB`其实就是`alter table t engine=innodb,ALGORITHM=inplace;`。 相对应的就是拷贝表的方式`alter table t engine=innodb,ALGORITHM=copy;`，这种方式在server层看来就是创建了临时表
- 如果我要给 InnoDB 表的一个字段加全文索引，写法是：`alter table t add FULLTEXT(field_name);`。这个过程是 inplace 的，但会阻塞增删改操作，是非 Online 的。

如果说这两个逻辑之间的关系是什么的话，可以概括为：
- **DDL 过程如果是 Online 的，就一定是 inplace 的**；
- 反过来未必，也就是说 **inplace 的 DDL，有可能不是 Online 的。**截止到 MySQL 8.0，添加全文索引（FULLTEXT index）和空间索引 (SPATIAL index) 就属于这种情况。

**optimize table、analyze table 和 alter table 的区别**：

- 从 MySQL 5.6 版本开始，alter table t engine = InnoDB（也就是 recreate）默认的就是online的流程了；
- analyze table t 其实不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程中加了 MDL 读锁；
- optimize table t 等于 recreate+analyze。

注意： Online DDL 的方式是可以考虑在业务低峰期使用的，而 MySQL 5.5 及之前的版本，这个命令是会阻塞 DML 的，这个你需要特别小心。

### 提问

假设现在有人碰到了一个“想要收缩表空间，结果适得其反”的情况，看上去是这样的：
1. 一个表 t 文件大小为 1TB；
2. 对这个表执行 alter table t engine=InnoDB；
3. 发现执行完成后，空间不仅没变小，还稍微大了一点儿，比如变成了 1.01TB。
你觉得可能是什么原因呢 

```
A:

1. 一种可能是：这个表，本身就已经没有空洞的了，比如说刚刚做过一次重建表操作。在 DDL 期间，如果刚好有外部的 DML 在执行，这期间可能会引入一些新的空洞。
2. 另一种是：在重建表的时候，InnoDB 不会把整张表占满，每个页留了 1/16 给后续的更新用。也就是说，其实重建表之后不是“最”紧凑的。
假如是这么一个过程：
a.将表 t 重建一次；
b.插入一部分数据，但是插入的这些数据，用掉了一部分的预留空间；
c.这种情况下，再重建一次表 t，就可能会出现问题中的现象。
```

## 14. count(*)这么慢，我该怎么办？ 

### 14.1 count(*)的实现方式

- MyISAM: 在磁盘上维护表中总行数。
    - 这说的是没有过滤条件的count(*)
- InnoDB: 需要把数据一行行读出来累积计数。
    - 即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的。
        - 这和 InnoDB 的事务设计有关系，可重复读是它默认的隔离级别，在代码上就是通过多版本并发控制，也就是 MVCC 来实现的。每一行记录都要判断自己是否对这个会话可见，因此对于 count(*) 请求来说，InnoDB 只好把数据一行一行地读出依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数。

InnoDB对count(*)的优化：
- MySQL **优化器会找到最小的那棵树来遍历**。 
    - （对于 count(*) 这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。） 
    - **在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。**   

`show table status`查到的`TABLE_ROWS`用于显示这个表当前有多少行，执行也快，可以用它查行数吗？
- 不可以 。 前面提到，索引统计的值是通过采样来估算的。实际上，TABLE_ROWS 就是从这个采样估算得来的，因此它也很不准。有多不准呢，官方文档说误差可能达到 40% 到 50%。所以，show table status 命令显示的行数也不能直接使用。    

**如果你现在有一个页面经常要显示交易系统的操作记录总数，到底应该怎么办呢？**
- 答案是，我们**只能自己计数**。
- 基本思路：你需要自己找一个地方，把操作记录表的行数存起来。

### 14.2 自己维护记录总数的方法

1. 用缓存系统（如Redis）保存计数
    - 问题：
        1. 缓存系统可能会丢失更新
            - 这个问题是有解的。比如，Redis 异常重启以后，到数据库里面单独执行一次 count(*) 获取真实的行数，再把这个值写回到 Redis 里就可以了。异常重启毕竟不是经常出现的情况，这一次全表扫描的成本，还是可以接受的。
        2. 即使缓存正常工作，这个值也是逻辑上不精确的
            - 并发系统里面，我们是无法精确控制不同线程的执行时刻的。因此可能redis内记录和数据库中实际行数不一致
2. 在**数据库保存计数**
   - 把这个计数直接放到数据库里单独的一张计数表 C 中
   - 这是正解
        - 这解决了崩溃丢失的问题，InnoDB 是支持崩溃恢复不丢数据的。
        - 通过将更新计数和插入记录放到一个事务中执行，保证了计数与实际记录行数的一致性

### 14.3 不同的count用法

对于InnoDB，在 select count(?) from t 这样的查询语句里面，count(*)、count(主键 id)、count(字段) 和 count(1) 等不同用法的性能，有哪些差别
-  count() 的语义。count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。
- 所以，**count(*)、count(主键 id) 和 count(1) 都表示返回满足条件的结果集的总行数**；而 **count(字段），则表示返回满足条件的数据行里面，参数“字段”不为 NULL 的总个数**。

至于分析性能差别的时候，你可以记住这么几个原则：
1. server 层要什么就给什么；
2. InnoDB 只给必要的值；
3. 现在的优化器只优化了 count(*) 的语义为“取行数”，其他“显而易见”的优化并没有做。

**对于 count(主键 id) 来说**，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。**server 层拿到 id 后，判断是不可能为空的，就按行累加**。

**对于 count(1) 来说**，InnoDB 引擎**遍历整张表，但不取值**。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。

单看这两个用法的差别的话，你能对比出来，**count(1) 执行得要比 count(主键 id) 快**。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。

**对于 count(字段) 来说：**
- 如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；
- 如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。
- 也就是前面的第一条原则，server 层要什么字段，InnoDB 就返回什么字段。

但是 **count(\*) 是例外，并不会把全部字段取出来，而是专门做了优化，不取值**。count(\*) 肯定不是 null，按行累加。

结论是：按照效率排序的话，count(字段) < count(主键 id) < count(1) ≈ count(\*)，所以我建议你，尽量使用 count(\*)。

### 提问

在刚刚讨论的方案中，我们用了事务来确保计数准确。由于事务可以保证中间结果不被别的事务读到，因此修改计数值和插入新记录的顺序是不影响逻辑结果的。但是，从并发系统性能的角度考虑，你觉得在这个事务序列里，应该先插入操作记录，还是应该先更新计数表呢？

```
A:

先插入记录。

因为只要是插入记录行，就需要更新计数表的那一个唯一的行，因此要尽量减少在计数行更新的写锁等待。而写锁必须在事务提交时才释放，因此，应该后更新计数行
```

## 16. “order by”是怎么工作的？

### 16.1 全字段排序

- 不给字段加索引的话，使用order by会全表扫描，所以要加索引。
- MySQL 会给每个线程分配一块内存用于排序，称为 `sort_buffer`。

以`select city,name,age from t where city='杭州' order by name limit 1000`为例，
"order by" 流程：
1.  初始化 sort_buffer，确定放入要返回的字段（city,name,age）。
2. 从索引中依次读取所有符合where查询条件的记录，将记录中各字段值填到sort_buffer
3. 对 sort_buffer 中的数据按照字段 name 做**快速排序**；
4. 按照排序结果取前 1000 行返回给客户端。

这个过程称为全字段排序。

第3步排序，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数 sort_buffer_size。
- 所需内存（要排序的数据量） < sort_buffer_size， 则在内存排序
- 否则，利用磁盘辅助文件进行外部排序（一般是归并排序）

### 16.2 rowid排序

全字段排序的问题在于：如果全字段太长，会导致需要的临时文件太大，验证影响排序性能。

可以通过设置`SET max_length_for_sort_data = 16;`来启用rowid排序。设置这个长度会用来判断全字段是否过长，全字段长度如果超过16（设定值）就启用rowid排序，

rowid排序下，只会取主键id和order by的字段来填到sort_buffer（不论是内存还是临时文件）、

这样，排好序、根据limit限制数量后，按所有结果的主键id再去表里查取需要的字段（回表）。

注意，结果集的概念是逻辑概念，实际上并不会维护在数据库内存中，而是直接返回给客户端

### 16.3 全字段排序 VS rowid排序

如果内存足够，全字段排序；不足再使用rowid排序

### 16.4 不是所有的order by都需要排序

MySQL 之所以需要生成临时表，并且在临时表上做排序操作，其原因是原来的数据都是无序的。

针对查询语句：
```
select city,name,age from t where city='杭州' order by name limit 1000 ;
```

如果能够保证从 city 这个索引（where条件字段）上取出来的行，天然就是按照 name （order by条件）递增排序的话，是不是就可以不用再排序了呢

办法就是创建index(city, name)联合索引，查出来的结果天然就是按name有序的

```sql
alter table t add index city_user(city, name);
```

### 16.5 进一步优化order by

在内存足够的前提下使用覆盖索引，可以不用回表，性能更好

```
alter table t add index city_user_age(city, name, age);
```

前面讲的三种order by使用explain语句时可以从Extra字段看出来：
- 正常order by（无论是全字段还是rowid排序）：Extra中有Using fileSort
- 使用联合索引避免排序的order by：Extra中没有Using fileSort
- 使用覆盖索引的order by：有Using index（表示覆盖索引）

### 提问

假设你的表里面已经有了 city_name(city, name) 这个联合索引，然后你要查杭州和苏州两个城市中所有的市民的姓名，并且按名字排序，显示前 100 条记录。如果 SQL 查询语句是这么写的 ：

```sql
mysql> select * from t where city in ('杭州'," 苏州 ") order by name limit 100;
```

那么，这个语句执行的时候会有排序过程吗，为什么？

如果业务端代码由你来开发，需要实现一个在数据库端不需要排序的方案，你会怎么实现
呢？

进一步地，如果有分页需求，要显示第 101 页，也就是说语句最后要改成 “limit
10000,100”， 你的实现方法又会是什么呢？

```
A:

1. 需要排序。因为要同时查两个城市，结果集就不再是天然有序的了

2. 如何避免数据库内排序。可以将sql拆成两句，分别查杭州和苏州的有序结果，再在客户端内存中进行合并两个有序数组

3. 一种办法是获得单个城市limit 10100的结果，再在客户端合并取10000~10100范围的结果，这种做法问题是返回给客户端的数据量可能很大；优化的方案，就是先只查name和id，在客户端求到所有符合要求的id，再去服务端查这些id对应的记录行的数据。
```

## 17. 如何正确地显示随机消息？

一张单词表（字段：id,word， 行数10000），每次随机取三个单词

### 17.1 order by rand() 使用内存临时表

```sql
mysql> select word from words order by rand() limit 3;
```

使用explain SQL可以看出，这句sql使用了临时表（Using temporary）和排序（Using filesort）

临时表
- 如果是InnoDB表，那么执行全字段排序会减少磁盘访问，因此会被优先选择。
- 如果是内存表，那么回表开销很小，不会多访问磁盘，优化器会选择用于排序的行更小的，所以会选rowid排序

执行流程：
1. 创建一个临时表。这个临时表使用的是 memory 引擎，表里有两个字段，第一个字段是 double 类型，为了后面描述方便，记为字段 R，第二个字段是 varchar(64) 类型，记为字段 W。并且，这个表没有建索引。
2. 从 words 表中，按主键顺序取出所有的 word 值。对于每一个 word 值，调用 rand() 函数生成一个大于 0 小于 1 的随机小数，并把这个随机小数和 word 分别存入临时表的 R 和 W 字段中，到此，扫描行数是 10000。
3. 现在临时表有 10000 行数据了，接下来你要在这个没有索引的内存临时表上，按照字段 R 排序。
4. 初始化 sort_buffer。sort_buffer 中有两个字段，一个是 double 类型，另一个是整型。
5. 从内存临时表中一行一行地取出 R 值和位置信息（我后面会和你解释这里为什么是“位置信息”），分别存入 sort_buffer 中的两个字段里。这个过程要对内存临时表做全表扫描，此时扫描行数增加 10000，变成了 20000。
6. 在 sort_buffer 中根据 R 的值进行排序。注意，这个过程没有涉及到表操作，所以不会增加扫描行数。
7. 排序完成后，取出前三个结果的位置信息，依次到内存临时表中取出 word 值，返回给客户端。这个过程中，访问了表的三行数据，总扫描行数变成了 20003。

“位置信息”
- 对于innodb表就是主键ID（如果没有自己指定主键ID，那么就是InnoDB自动生成的6B的rowid）
- MEMORY 引擎不是索引组织表。在这个例子里面，你可以认为它就是一个数组。因此，这个 rowid 其实就是数组的下标。

小结一下：**order by rand() 使用了内存临时表，内存临时表排序的时候使用了 rowid 排序方法**。

### 17.2 磁盘临时表

**不是所有的临时表都是内存表**

`tmp_table_size` 这个配置限制了内存临时表的大小，默认值是 16M。如果临时表大小超过了 `tmp_table_size`，那么内存临时表就会转成磁盘临时表。

磁盘临时表使用的引擎默认是 InnoDB，是由参数 `internal_tmp_disk_storage_engine` 控制的。

当使用磁盘临时表的时候，对应的就是一个没有显式索引的 InnoDB 表的排序过程。

为了复现这个过程，我把 tmp_table_size 设置成 1024，把 sort_buffer_size 设置成 32768, 把 max_length_for_sort_data 设置成 16。

```sql
set tmp_table_size=1024;
set sort_buffer_size=32768;
set max_length_for_sort_data=16;
/* 打开 optimizer_trace，只对本线程有效 */
SET optimizer_trace='enabled=on'; 
/* 执行语句 */
select word from words order by rand() limit 3;
/* 查看 OPTIMIZER_TRACE 输出 */
SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G
```
![mysql45-17-1](/images/mysql45-17-1.png)

OPTIMIZER_TRACE 的结果显示：使用rowid排序方式，但没有使用临时文件。

这是因为没有使用归并排序算法，而采用是 MySQL 5.6 版本引入的一个新的排序算法，即：**优先队列排序算法**。

这是因为只需要取排序前三个，没必要把所有元素排好序，因此优化器会选用优先队列排序而非归并排序
    - 这是看limit x，如果x较小，建的堆大小为x，也较小，占内存较少，这种时候优化器会更倾向于优先队列；否则倾向于归并排序

总之，不论是使用哪种类型的临时表，order by rand() 这种写法都会让计算过程非常复杂，需要大量的扫描行数，因此排序过程的资源消耗也会很大。

### 17.3 随机排序算法

随机算法1：

如果只随机选择 1 个 word 值，可以怎么做呢？思路上是这样的：
1. 取得这个表的主键 id 的最大值 M 和最小值 N;
2. 用随机函数生成一个最大值到最小值之间的数 X = (M-N)*rand() + N;
3. 取不小于 X 的第一个 ID 的行。

实际上，这个算法本身并不严格满足题目的随机要求，因为 ID 中间可能有空洞，因此选择不同行的概率不一样，不是真正的随机。

随机算法1只需要扫描3行

随机算法2：

为了得到严格随机的结果，你可以用下面这个流程:
1. 取得整个表的行数，并记为 C。
2. 取得 Y = floor(C * rand())。 floor 函数在这里的作用，就是取整数部分。
3. 再用 limit Y,1 取得一行。

随机算法2需要扫描C+Y+1行，还是比order by ramd()快很多

随机算法3：

如果我们按照随机算法 2 的思路，要随机取 3 个 word 值呢？你可以这么做：
1. 取得整个表的行数，记为 C；
2. 根据相同的随机方法得到 Y1、Y2、Y3；
3. 再执行三个 limit Y, 1 语句得到三行数据。

随机算法 3 的总扫描行数是 C+(Y1+1)+(Y2+1)+(Y3+1)

### 总结

在实际应用的过程中，比较规范的用法就是：尽量将业务逻辑写在业务代码中，让数据库只做“读写数据”的事情。

### 提问

上面的随机算法 3 的总扫描行数是 C+(Y1+1)+(Y2+1)+(Y3+1)，实际上它还是可以继续优化，来进一步减少扫描行数的。

我的问题是，如果你是这个需求的开发人员，你会怎么做，来减少扫描行数呢？说说你的方案，并说明你的方案需要的扫描行数。

```
A:

1. 一种方法是取三个随机数，但是只拿连续三个单词。具体来说：取 Y1、Y2 和 Y3 里面最大的一个数，记为 M，最小的一个数记为 N，然后执行：
mysql> select * from t limit N, M-N+1;
再加上取整个表总行数的 C 行，这个方案的扫描行数总共只需要 C+M+1 行。

2. 可以先取回 id 值，在应用中确定了三个 id 值以后，再执行三次 where id=X 的语句也是可以的。
```

## 18. 为什么这些SQL语句逻辑相同，性能却差异巨大？

### 18.1 条件字段函数操作

**对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。**
- 优化器在个问题上确实有“偷懒”行为，即使是对于不改变有序性的函数，也不会考虑使用索引。

### 18.2 隐式类型转换

查询语句中如果有字符串与数字比较（或者其他两种类型），那么会触发隐式类型转换，也就是函数操作，也会引发索引失效

### 18.3 隐式字符编码转换

两个不同字符集编码的表在连接(join)时，关联字段的索引会失效

假如一个表是utf8编码，一个是utf8mb4编码， 那么会在关联时将utf8字符串转换为utf8mb4。（utf8mb4是utf8的超集）

连接过程中要求在被驱动表的索引字段上加函数操作，是直接导致对被驱动表做全表扫描的原因。

### 总结

对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。

## 19. 为什么我只查一行的语句，也执行这么慢？

需要说明的是，如果 MySQL 数据库本身就有很大的压力，导致数据库服务器 CPU 占用率很高或 ioutil（IO 利用率）很高，这种情况下所有语句的执行都有可能变慢，不属于我们今天的讨论范围。

### 19.1 查询长时间不返回

**大概率是表被锁住了**。 诊断步骤：
1. 执行`show processlist` 命令，看看当前语句处于什么状态。
2. 然后我们再针对每种状态，去分析它们产生的原因、如何复现，以及如何处理。

情况一：等MDL锁

show processlist 查到 `Status` 为 `Waiting for table metadata lock `

这个状态表示的是，现在有一个线程正在表 t 上请求或者持有 MDL 写锁，把 select 语句堵住了。

这类问题的处理方式，**就是找到谁持有 MDL 写锁，然后把它 kill 掉**。



